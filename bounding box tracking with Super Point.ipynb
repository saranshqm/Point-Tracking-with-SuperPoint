{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on any random image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Processing Video Input.\n",
      "==> Loading pre-trained network.\n",
      "==> Successfully loaded pre-trained network.\n",
      "==> Running Demo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SARANSH\\Anaconda3\\envs\\saransh\\lib\\site-packages\\torch\\nn\\functional.py:3226: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\"Default grid_sample and affine_grid behavior has changed \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed image 1 (net+post_process: 0.73 FPS, total: 0.20 FPS).\n",
      "Processed image 2 (net+post_process: 25.71 FPS, total: 2.00 FPS).\n",
      "Processed image 3 (net+post_process: 27.10 FPS, total: 11.94 FPS).\n",
      "Processed image 4 (net+post_process: 27.85 FPS, total: 11.39 FPS).\n",
      "Processed image 5 (net+post_process: 23.32 FPS, total: 9.55 FPS).\n",
      "Processed image 6 (net+post_process: 23.87 FPS, total: 11.66 FPS).\n",
      "Processed image 7 (net+post_process: 20.05 FPS, total: 9.93 FPS).\n",
      "Processed image 8 (net+post_process: 17.29 FPS, total: 7.65 FPS).\n",
      "Processed image 9 (net+post_process: 21.80 FPS, total: 9.50 FPS).\n",
      "Processed image 10 (net+post_process: 20.89 FPS, total: 8.79 FPS).\n",
      "Processed image 11 (net+post_process: 20.67 FPS, total: 8.99 FPS).\n",
      "Processed image 12 (net+post_process: 21.14 FPS, total: 8.54 FPS).\n",
      "Processed image 13 (net+post_process: 21.34 FPS, total: 9.37 FPS).\n",
      "Processed image 14 (net+post_process: 21.80 FPS, total: 9.88 FPS).\n",
      "Processed image 15 (net+post_process: 21.84 FPS, total: 8.09 FPS).\n",
      "Processed image 16 (net+post_process: 18.92 FPS, total: 9.97 FPS).\n",
      "Processed image 17 (net+post_process: 20.05 FPS, total: 10.34 FPS).\n",
      "Processed image 18 (net+post_process: 22.28 FPS, total: 10.13 FPS).\n",
      "Processed image 19 (net+post_process: 17.59 FPS, total: 8.38 FPS).\n",
      "Processed image 20 (net+post_process: 22.28 FPS, total: 8.87 FPS).\n",
      "Processed image 21 (net+post_process: 18.57 FPS, total: 8.99 FPS).\n",
      "Processed image 22 (net+post_process: 18.23 FPS, total: 8.90 FPS).\n",
      "Processed image 23 (net+post_process: 17.91 FPS, total: 10.67 FPS).\n",
      "Processed image 24 (net+post_process: 17.91 FPS, total: 9.73 FPS).\n",
      "Processed image 25 (net+post_process: 21.88 FPS, total: 12.09 FPS).\n",
      "Processed image 26 (net+post_process: 18.88 FPS, total: 11.78 FPS).\n",
      "Processed image 27 (net+post_process: 21.80 FPS, total: 12.08 FPS).\n",
      "Processed image 28 (net+post_process: 23.32 FPS, total: 11.94 FPS).\n",
      "Processed image 29 (net+post_process: 23.87 FPS, total: 11.09 FPS).\n",
      "Processed image 30 (net+post_process: 21.30 FPS, total: 9.81 FPS).\n",
      "Processed image 31 (net+post_process: 21.29 FPS, total: 11.12 FPS).\n",
      "Processed image 32 (net+post_process: 23.87 FPS, total: 9.83 FPS).\n",
      "Processed image 33 (net+post_process: 23.87 FPS, total: 12.69 FPS).\n",
      "Processed image 34 (net+post_process: 25.07 FPS, total: 11.39 FPS).\n",
      "Processed image 35 (net+post_process: 16.99 FPS, total: 8.52 FPS).\n",
      "Processed image 36 (net+post_process: 24.46 FPS, total: 12.23 FPS).\n",
      "Processed image 37 (net+post_process: 18.92 FPS, total: 9.12 FPS).\n",
      "Processed image 38 (net+post_process: 19.28 FPS, total: 9.37 FPS).\n",
      "Processed image 39 (net+post_process: 20.46 FPS, total: 10.77 FPS).\n",
      "Processed image 40 (net+post_process: 17.43 FPS, total: 9.04 FPS).\n",
      "Processed image 41 (net+post_process: 24.45 FPS, total: 11.14 FPS).\n",
      "Processed image 42 (net+post_process: 20.46 FPS, total: 8.95 FPS).\n",
      "Processed image 43 (net+post_process: 20.46 FPS, total: 9.33 FPS).\n",
      "Processed image 44 (net+post_process: 18.80 FPS, total: 9.70 FPS).\n",
      "Processed image 45 (net+post_process: 16.71 FPS, total: 6.91 FPS).\n",
      "Processed image 46 (net+post_process: 18.39 FPS, total: 9.34 FPS).\n",
      "Processed image 47 (net+post_process: 18.57 FPS, total: 9.24 FPS).\n",
      "Processed image 48 (net+post_process: 17.57 FPS, total: 8.35 FPS).\n",
      "Processed image 49 (net+post_process: 15.67 FPS, total: 8.85 FPS).\n",
      "Processed image 50 (net+post_process: 17.29 FPS, total: 9.88 FPS).\n",
      "Processed image 51 (net+post_process: 16.99 FPS, total: 8.43 FPS).\n",
      "Processed image 52 (net+post_process: 17.74 FPS, total: 9.41 FPS).\n",
      "Processed image 53 (net+post_process: 20.05 FPS, total: 9.03 FPS).\n",
      "Processed image 54 (net+post_process: 19.41 FPS, total: 10.16 FPS).\n",
      "Processed image 55 (net+post_process: 17.91 FPS, total: 8.80 FPS).\n",
      "Processed image 56 (net+post_process: 17.59 FPS, total: 9.03 FPS).\n",
      "Processed image 57 (net+post_process: 21.08 FPS, total: 10.38 FPS).\n",
      "Processed image 58 (net+post_process: 22.28 FPS, total: 9.83 FPS).\n",
      "Processed image 59 (net+post_process: 23.14 FPS, total: 12.55 FPS).\n",
      "Processed image 60 (net+post_process: 17.58 FPS, total: 9.83 FPS).\n",
      "Processed image 61 (net+post_process: 17.59 FPS, total: 10.13 FPS).\n",
      "Processed image 62 (net+post_process: 20.46 FPS, total: 11.53 FPS).\n",
      "Processed image 63 (net+post_process: 21.33 FPS, total: 10.78 FPS).\n",
      "Processed image 64 (net+post_process: 20.05 FPS, total: 9.55 FPS).\n",
      "Processed image 65 (net+post_process: 19.28 FPS, total: 11.20 FPS).\n",
      "Processed image 66 (net+post_process: 17.38 FPS, total: 7.85 FPS).\n",
      "Processed image 67 (net+post_process: 18.92 FPS, total: 10.26 FPS).\n",
      "Processed image 68 (net+post_process: 23.32 FPS, total: 10.34 FPS).\n",
      "Processed image 69 (net+post_process: 18.23 FPS, total: 8.22 FPS).\n",
      "Processed image 70 (net+post_process: 23.32 FPS, total: 11.14 FPS).\n",
      "Processed image 71 (net+post_process: 18.92 FPS, total: 8.81 FPS).\n",
      "Processed image 72 (net+post_process: 19.47 FPS, total: 9.24 FPS).\n",
      "Processed image 73 (net+post_process: 17.90 FPS, total: 9.64 FPS).\n",
      "Processed image 74 (net+post_process: 18.46 FPS, total: 10.30 FPS).\n",
      "Processed image 75 (net+post_process: 18.57 FPS, total: 9.59 FPS).\n",
      "Processed image 76 (net+post_process: 15.43 FPS, total: 8.57 FPS).\n",
      "Processed image 77 (net+post_process: 19.28 FPS, total: 9.55 FPS).\n",
      "Processed image 78 (net+post_process: 16.83 FPS, total: 7.92 FPS).\n",
      "Processed image 79 (net+post_process: 23.32 FPS, total: 11.14 FPS).\n",
      "Processed image 80 (net+post_process: 16.99 FPS, total: 8.95 FPS).\n",
      "Processed image 81 (net+post_process: 23.87 FPS, total: 10.02 FPS).\n",
      "Processed image 82 (net+post_process: 20.05 FPS, total: 8.28 FPS).\n",
      "Processed image 83 (net+post_process: 23.87 FPS, total: 11.27 FPS).\n",
      "Processed image 84 (net+post_process: 19.84 FPS, total: 8.18 FPS).\n",
      "Processed image 85 (net+post_process: 21.09 FPS, total: 9.68 FPS).\n",
      "Processed image 86 (net+post_process: 20.46 FPS, total: 9.46 FPS).\n",
      "Processed image 87 (net+post_process: 19.28 FPS, total: 9.55 FPS).\n",
      "Processed image 88 (net+post_process: 18.92 FPS, total: 8.67 FPS).\n",
      "Processed image 89 (net+post_process: 20.46 FPS, total: 9.03 FPS).\n",
      "Processed image 90 (net+post_process: 19.66 FPS, total: 8.95 FPS).\n",
      "Processed image 91 (net+post_process: 22.79 FPS, total: 11.80 FPS).\n",
      "Processed image 92 (net+post_process: 17.90 FPS, total: 9.46 FPS).\n",
      "Processed image 93 (net+post_process: 23.59 FPS, total: 9.69 FPS).\n",
      "Processed image 94 (net+post_process: 19.28 FPS, total: 9.10 FPS).\n",
      "Processed image 95 (net+post_process: 22.79 FPS, total: 12.15 FPS).\n",
      "Processed image 96 (net+post_process: 17.90 FPS, total: 8.50 FPS).\n",
      "Processed image 97 (net+post_process: 15.54 FPS, total: 8.25 FPS).\n",
      "Processed image 98 (net+post_process: 20.89 FPS, total: 9.64 FPS).\n",
      "Processed image 99 (net+post_process: 20.25 FPS, total: 8.62 FPS).\n",
      "Processed image 100 (net+post_process: 18.57 FPS, total: 8.09 FPS).\n",
      "Processed image 101 (net+post_process: 16.46 FPS, total: 8.79 FPS).\n",
      "Processed image 102 (net+post_process: 20.05 FPS, total: 11.02 FPS).\n",
      "Processed image 103 (net+post_process: 19.28 FPS, total: 8.95 FPS).\n",
      "Processed image 104 (net+post_process: 16.44 FPS, total: 7.98 FPS).\n",
      "Processed image 105 (net+post_process: 20.89 FPS, total: 9.70 FPS).\n",
      "Processed image 106 (net+post_process: 14.32 FPS, total: 7.03 FPS).\n",
      "Processed image 107 (net+post_process: 18.57 FPS, total: 9.73 FPS).\n",
      "Processed image 108 (net+post_process: 19.28 FPS, total: 10.44 FPS).\n",
      "Processed image 109 (net+post_process: 18.71 FPS, total: 9.68 FPS).\n",
      "Processed image 110 (net+post_process: 16.71 FPS, total: 10.34 FPS).\n",
      "Processed image 111 (net+post_process: 19.66 FPS, total: 9.13 FPS).\n",
      "Processed image 112 (net+post_process: 19.24 FPS, total: 10.65 FPS).\n",
      "Processed image 113 (net+post_process: 22.79 FPS, total: 10.44 FPS).\n",
      "Processed image 114 (net+post_process: 19.66 FPS, total: 11.66 FPS).\n",
      "Processed image 115 (net+post_process: 21.34 FPS, total: 9.83 FPS).\n",
      "Processed image 116 (net+post_process: 19.28 FPS, total: 9.40 FPS).\n",
      "Processed image 117 (net+post_process: 17.90 FPS, total: 8.85 FPS).\n",
      "Processed image 118 (net+post_process: 23.32 FPS, total: 11.10 FPS).\n",
      "Processed image 119 (net+post_process: 22.79 FPS, total: 12.85 FPS).\n",
      "Processed image 120 (net+post_process: 20.26 FPS, total: 9.42 FPS).\n",
      "Processed image 121 (net+post_process: 18.92 FPS, total: 11.14 FPS).\n",
      "Processed image 122 (net+post_process: 21.80 FPS, total: 10.78 FPS).\n",
      "Processed image 123 (net+post_process: 17.59 FPS, total: 11.02 FPS).\n",
      "Processed image 124 (net+post_process: 17.90 FPS, total: 11.39 FPS).\n",
      "Processed image 125 (net+post_process: 20.05 FPS, total: 9.93 FPS).\n",
      "Processed image 126 (net+post_process: 16.59 FPS, total: 10.51 FPS).\n",
      "Processed image 127 (net+post_process: 23.87 FPS, total: 13.55 FPS).\n",
      "Quitting, 'q' pressed.\n",
      "==> Finshed Demo.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "# Stub to warn about opencv version.\n",
    "if int(cv2.__version__[0]) < 3: # pragma: no cover\n",
    "  print('Warning: OpenCV 3 is not installed')\n",
    "\n",
    "# Jet colormap for visualization.\n",
    "myjet = np.array([[0.        , 0.        , 0.5       ],\n",
    "                  [0.        , 0.        , 0.99910873],\n",
    "                  [0.        , 0.37843137, 1.        ],\n",
    "                  [0.        , 0.83333333, 1.        ],\n",
    "                  [0.30044276, 1.        , 0.66729918],\n",
    "                  [0.66729918, 1.        , 0.30044276],\n",
    "                  [1.        , 0.90123457, 0.        ],\n",
    "                  [1.        , 0.48002905, 0.        ],\n",
    "                  [0.99910873, 0.07334786, 0.        ],\n",
    "                  [0.5       , 0.        , 0.        ]])\n",
    "\n",
    "class SuperPointNet(torch.nn.Module):\n",
    "  \"\"\" Pytorch definition of SuperPoint Network. \"\"\"\n",
    "  def __init__(self):\n",
    "    super(SuperPointNet, self).__init__()\n",
    "    self.relu = torch.nn.ReLU(inplace=True)\n",
    "    self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    c1, c2, c3, c4, c5, d1 = 64, 64, 128, 128, 256, 256\n",
    "    # Shared Encoder.\n",
    "    self.conv1a = torch.nn.Conv2d(1, c1, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv1b = torch.nn.Conv2d(c1, c1, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv2a = torch.nn.Conv2d(c1, c2, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv2b = torch.nn.Conv2d(c2, c2, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv3a = torch.nn.Conv2d(c2, c3, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv3b = torch.nn.Conv2d(c3, c3, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv4a = torch.nn.Conv2d(c3, c4, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv4b = torch.nn.Conv2d(c4, c4, kernel_size=3, stride=1, padding=1)\n",
    "    # Detector Head.\n",
    "    self.convPa = torch.nn.Conv2d(c4, c5, kernel_size=3, stride=1, padding=1)\n",
    "    self.convPb = torch.nn.Conv2d(c5, 65, kernel_size=1, stride=1, padding=0)\n",
    "    # Descriptor Head.\n",
    "    self.convDa = torch.nn.Conv2d(c4, c5, kernel_size=3, stride=1, padding=1)\n",
    "    self.convDb = torch.nn.Conv2d(c5, d1, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\" Forward pass that jointly computes unprocessed point and descriptor\n",
    "    tensors.\n",
    "    Input\n",
    "      x: Image pytorch tensor shaped N x 1 x H x W.\n",
    "    Output\n",
    "      semi: Output point pytorch tensor shaped N x 65 x H/8 x W/8.\n",
    "      desc: Output descriptor pytorch tensor shaped N x 256 x H/8 x W/8.\n",
    "    \"\"\"\n",
    "    # Shared Encoder.\n",
    "    x = self.relu(self.conv1a(x))\n",
    "    x = self.relu(self.conv1b(x))\n",
    "    x = self.pool(x)\n",
    "    x = self.relu(self.conv2a(x))\n",
    "    x = self.relu(self.conv2b(x))\n",
    "    x = self.pool(x)\n",
    "    x = self.relu(self.conv3a(x))\n",
    "    x = self.relu(self.conv3b(x))\n",
    "    x = self.pool(x)\n",
    "    x = self.relu(self.conv4a(x))\n",
    "    x = self.relu(self.conv4b(x))\n",
    "    # Detector Head.\n",
    "    cPa = self.relu(self.convPa(x))\n",
    "    semi = self.convPb(cPa)\n",
    "    # Descriptor Head.\n",
    "    cDa = self.relu(self.convDa(x))\n",
    "    desc = self.convDb(cDa)\n",
    "    dn = torch.norm(desc, p=2, dim=1) # Compute the norm.\n",
    "    desc = desc.div(torch.unsqueeze(dn, 1)) # Divide by norm to normalize.\n",
    "    return semi, desc\n",
    "\n",
    "\n",
    "class SuperPointFrontend(object):\n",
    "  \"\"\" Wrapper around pytorch net to help with pre and post image processing. \"\"\"\n",
    "  def __init__(self, weights_path, nms_dist, conf_thresh, nn_thresh,\n",
    "               cuda=False):\n",
    "    self.name = 'SuperPoint'\n",
    "    self.cuda = cuda\n",
    "    self.nms_dist = nms_dist\n",
    "    self.conf_thresh = conf_thresh\n",
    "    self.nn_thresh = nn_thresh # L2 descriptor distance for good match.\n",
    "    self.cell = 8 # Size of each output cell. Keep this fixed.\n",
    "    self.border_remove = 4 # Remove points this close to the border.\n",
    "\n",
    "    # Load the network in inference mode.\n",
    "    self.net = SuperPointNet()\n",
    "    if cuda:\n",
    "      # Train on GPU, deploy on GPU.\n",
    "      self.net.load_state_dict(torch.load(weights_path))\n",
    "      self.net = self.net.cuda()\n",
    "    else:\n",
    "      # Train on GPU, deploy on CPU.\n",
    "      self.net.load_state_dict(torch.load(weights_path,\n",
    "                               map_location=lambda storage, loc: storage))\n",
    "    self.net.eval()\n",
    "\n",
    "  def nms_fast(self, in_corners, H, W, dist_thresh):\n",
    "    \"\"\"\n",
    "    Run a faster approximate Non-Max-Suppression on numpy corners shaped:\n",
    "      3xN [x_i,y_i,conf_i]^T\n",
    "  \n",
    "    Algo summary: Create a grid sized HxW. Assign each corner location a 1, rest\n",
    "    are zeros. Iterate through all the 1's and convert them either to -1 or 0.\n",
    "    Suppress points by setting nearby values to 0.\n",
    "  \n",
    "    Grid Value Legend:\n",
    "    -1 : Kept.\n",
    "     0 : Empty or suppressed.\n",
    "     1 : To be processed (converted to either kept or supressed).\n",
    "  \n",
    "    NOTE: The NMS first rounds points to integers, so NMS distance might not\n",
    "    be exactly dist_thresh. It also assumes points are within image boundaries.\n",
    "  \n",
    "    Inputs\n",
    "      in_corners - 3xN numpy array with corners [x_i, y_i, confidence_i]^T.\n",
    "      H - Image height.\n",
    "      W - Image width.\n",
    "      dist_thresh - Distance to suppress, measured as an infinty norm distance.\n",
    "    Returns\n",
    "      nmsed_corners - 3xN numpy matrix with surviving corners.\n",
    "      nmsed_inds - N length numpy vector with surviving corner indices.\n",
    "    \"\"\"\n",
    "    grid = np.zeros((H, W)).astype(int) # Track NMS data.\n",
    "    inds = np.zeros((H, W)).astype(int) # Store indices of points.\n",
    "    # Sort by confidence and round to nearest int.\n",
    "    inds1 = np.argsort(-in_corners[2,:])\n",
    "    corners = in_corners[:,inds1]\n",
    "    rcorners = corners[:2,:].round().astype(int) # Rounded corners.\n",
    "    # Check for edge case of 0 or 1 corners.\n",
    "    if rcorners.shape[1] == 0:\n",
    "      return np.zeros((3,0)).astype(int), np.zeros(0).astype(int)\n",
    "    if rcorners.shape[1] == 1:\n",
    "      out = np.vstack((rcorners, in_corners[2])).reshape(3,1)\n",
    "      return out, np.zeros((1)).astype(int)\n",
    "    # Initialize the grid.\n",
    "    for i, rc in enumerate(rcorners.T):\n",
    "      grid[rcorners[1,i], rcorners[0,i]] = 1\n",
    "      inds[rcorners[1,i], rcorners[0,i]] = i\n",
    "    # Pad the border of the grid, so that we can NMS points near the border.\n",
    "    pad = dist_thresh\n",
    "    grid = np.pad(grid, ((pad,pad), (pad,pad)), mode='constant')\n",
    "    # Iterate through points, highest to lowest conf, suppress neighborhood.\n",
    "    count = 0\n",
    "    for i, rc in enumerate(rcorners.T):\n",
    "      # Account for top and left padding.\n",
    "      pt = (rc[0]+pad, rc[1]+pad)\n",
    "      if grid[pt[1], pt[0]] == 1: # If not yet suppressed.\n",
    "        grid[pt[1]-pad:pt[1]+pad+1, pt[0]-pad:pt[0]+pad+1] = 0\n",
    "        grid[pt[1], pt[0]] = -1\n",
    "        count += 1\n",
    "    # Get all surviving -1's and return sorted array of remaining corners.\n",
    "    keepy, keepx = np.where(grid==-1)\n",
    "    keepy, keepx = keepy - pad, keepx - pad\n",
    "    inds_keep = inds[keepy, keepx]\n",
    "    out = corners[:, inds_keep]\n",
    "    values = out[-1, :]\n",
    "    inds2 = np.argsort(-values)\n",
    "    out = out[:, inds2]\n",
    "    out_inds = inds1[inds_keep[inds2]]\n",
    "    return out, out_inds\n",
    "\n",
    "  def run(self, img):\n",
    "    \"\"\" Process a numpy image to extract points and descriptors.\n",
    "    Input\n",
    "      img - HxW numpy float32 input image in range [0,1].\n",
    "    Output\n",
    "      corners - 3xN numpy array with corners [x_i, y_i, confidence_i]^T.\n",
    "      desc - 256xN numpy array of corresponding unit normalized descriptors.\n",
    "      heatmap - HxW numpy heatmap in range [0,1] of point confidences.\n",
    "      \"\"\"\n",
    "    assert img.ndim == 2, 'Image must be grayscale.'\n",
    "    assert img.dtype == np.float32, 'Image must be float32.'\n",
    "    H, W = img.shape[0], img.shape[1]\n",
    "    inp = img.copy()\n",
    "    inp = (inp.reshape(1, H, W))\n",
    "    inp = torch.from_numpy(inp)\n",
    "    inp = torch.autograd.Variable(inp).view(1, 1, H, W)\n",
    "    if self.cuda:\n",
    "      inp = inp.cuda()\n",
    "    # Forward pass of network.\n",
    "    outs = self.net.forward(inp)\n",
    "    semi, coarse_desc = outs[0], outs[1]\n",
    "    # Convert pytorch -> numpy.\n",
    "    semi = semi.data.cpu().numpy().squeeze()\n",
    "    # --- Process points.\n",
    "    dense = np.exp(semi) # Softmax.\n",
    "    dense = dense / (np.sum(dense, axis=0)+.00001) # Should sum to 1.\n",
    "    # Remove dustbin.\n",
    "    nodust = dense[:-1, :, :]\n",
    "    # Reshape to get full resolution heatmap.\n",
    "    Hc = int(H / self.cell)\n",
    "    Wc = int(W / self.cell)\n",
    "    nodust = nodust.transpose(1, 2, 0)\n",
    "    heatmap = np.reshape(nodust, [Hc, Wc, self.cell, self.cell])\n",
    "    heatmap = np.transpose(heatmap, [0, 2, 1, 3])\n",
    "    heatmap = np.reshape(heatmap, [Hc*self.cell, Wc*self.cell])\n",
    "    xs, ys = np.where(heatmap >= self.conf_thresh) # Confidence threshold.\n",
    "    if len(xs) == 0:\n",
    "      return np.zeros((3, 0)), None, None\n",
    "    pts = np.zeros((3, len(xs))) # Populate point data sized 3xN.\n",
    "    pts[0, :] = ys\n",
    "    pts[1, :] = xs\n",
    "    pts[2, :] = heatmap[xs, ys]\n",
    "    pts, _ = self.nms_fast(pts, H, W, dist_thresh=self.nms_dist) # Apply NMS.\n",
    "    inds = np.argsort(pts[2,:])\n",
    "    pts = pts[:,inds[::-1]] # Sort by confidence.\n",
    "    # Remove points along border.\n",
    "    bord = self.border_remove\n",
    "    toremoveW = np.logical_or(pts[0, :] < bord, pts[0, :] >= (W-bord))\n",
    "    toremoveH = np.logical_or(pts[1, :] < bord, pts[1, :] >= (H-bord))\n",
    "    toremove = np.logical_or(toremoveW, toremoveH)\n",
    "    pts = pts[:, ~toremove]\n",
    "    # --- Process descriptor.\n",
    "    D = coarse_desc.shape[1]\n",
    "    if pts.shape[1] == 0:\n",
    "      desc = np.zeros((D, 0))\n",
    "    else:\n",
    "      # Interpolate into descriptor map using 2D point locations.\n",
    "      samp_pts = torch.from_numpy(pts[:2, :].copy())\n",
    "      samp_pts[0, :] = (samp_pts[0, :] / (float(W)/2.)) - 1.\n",
    "      samp_pts[1, :] = (samp_pts[1, :] / (float(H)/2.)) - 1.\n",
    "      samp_pts = samp_pts.transpose(0, 1).contiguous()\n",
    "      samp_pts = samp_pts.view(1, 1, -1, 2)\n",
    "      samp_pts = samp_pts.float()\n",
    "      if self.cuda:\n",
    "        samp_pts = samp_pts.cuda()\n",
    "      desc = torch.nn.functional.grid_sample(coarse_desc, samp_pts)\n",
    "      desc = desc.data.cpu().numpy().reshape(D, -1)\n",
    "      desc /= np.linalg.norm(desc, axis=0)[np.newaxis, :]\n",
    "    return pts, desc, heatmap\n",
    "\n",
    "\n",
    "class PointTracker(object):\n",
    "  \"\"\" Class to manage a fixed memory of points and descriptors that enables\n",
    "  sparse optical flow point tracking.\n",
    "\n",
    "  Internally, the tracker stores a 'tracks' matrix sized M x (2+L), of M\n",
    "  tracks with maximum length L, where each row corresponds to:\n",
    "  row_m = [track_id_m, avg_desc_score_m, point_id_0_m, ..., point_id_L-1_m].\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, max_length, nn_thresh):\n",
    "    if max_length < 2:\n",
    "      raise ValueError('max_length must be greater than or equal to 2.')\n",
    "    self.maxl = max_length\n",
    "    self.nn_thresh = nn_thresh\n",
    "    self.all_pts = []\n",
    "    for n in range(self.maxl):\n",
    "      self.all_pts.append(np.zeros((2, 0)))\n",
    "    self.last_desc = None\n",
    "    self.tracks = np.zeros((0, self.maxl+2))\n",
    "    self.track_count = 0\n",
    "    self.max_score = 9999\n",
    "\n",
    "  def nn_match_two_way(self, desc1, desc2, nn_thresh):\n",
    "    \"\"\"\n",
    "    Performs two-way nearest neighbor matching of two sets of descriptors, such\n",
    "    that the NN match from descriptor A->B must equal the NN match from B->A.\n",
    "\n",
    "    Inputs:\n",
    "      desc1 - NxM numpy matrix of N corresponding M-dimensional descriptors.\n",
    "      desc2 - NxM numpy matrix of N corresponding M-dimensional descriptors.\n",
    "      nn_thresh - Optional descriptor distance below which is a good match.\n",
    "\n",
    "    Returns:\n",
    "      matches - 3xL numpy array, of L matches, where L <= N and each column i is\n",
    "                a match of two descriptors, d_i in image 1 and d_j' in image 2:\n",
    "                [d_i index, d_j' index, match_score]^T\n",
    "    \"\"\"\n",
    "    assert desc1.shape[0] == desc2.shape[0]\n",
    "    if desc1.shape[1] == 0 or desc2.shape[1] == 0:\n",
    "      return np.zeros((3, 0))\n",
    "    if nn_thresh < 0.0:\n",
    "      raise ValueError('\\'nn_thresh\\' should be non-negative')\n",
    "    # Compute L2 distance. Easy since vectors are unit normalized.\n",
    "    dmat = np.dot(desc1.T, desc2)\n",
    "    dmat = np.sqrt(2-2*np.clip(dmat, -1, 1))\n",
    "    # Get NN indices and scores.\n",
    "    idx = np.argmin(dmat, axis=1)\n",
    "    scores = dmat[np.arange(dmat.shape[0]), idx]\n",
    "    # Threshold the NN matches.\n",
    "    keep = scores < nn_thresh\n",
    "    # Check if nearest neighbor goes both directions and keep those.\n",
    "    idx2 = np.argmin(dmat, axis=0)\n",
    "    keep_bi = np.arange(len(idx)) == idx2[idx]\n",
    "    keep = np.logical_and(keep, keep_bi)\n",
    "    idx = idx[keep]\n",
    "    scores = scores[keep]\n",
    "    # Get the surviving point indices.\n",
    "    m_idx1 = np.arange(desc1.shape[1])[keep]\n",
    "    m_idx2 = idx\n",
    "    # Populate the final 3xN match data structure.\n",
    "    matches = np.zeros((3, int(keep.sum())))\n",
    "    matches[0, :] = m_idx1\n",
    "    matches[1, :] = m_idx2\n",
    "    matches[2, :] = scores\n",
    "    return matches\n",
    "\n",
    "  def get_offsets(self):\n",
    "    \"\"\" Iterate through list of points and accumulate an offset value. Used to\n",
    "    index the global point IDs into the list of points.\n",
    "\n",
    "    Returns\n",
    "      offsets - N length array with integer offset locations.\n",
    "    \"\"\"\n",
    "    # Compute id offsets.\n",
    "    offsets = []\n",
    "    offsets.append(0)\n",
    "    for i in range(len(self.all_pts)-1): # Skip last camera size, not needed.\n",
    "      offsets.append(self.all_pts[i].shape[1])\n",
    "    offsets = np.array(offsets)\n",
    "    offsets = np.cumsum(offsets)\n",
    "    return offsets\n",
    "\n",
    "  def update(self, pts, desc):\n",
    "    \"\"\" Add a new set of point and descriptor observations to the tracker.\n",
    "\n",
    "    Inputs\n",
    "      pts - 3xN numpy array of 2D point observations.\n",
    "      desc - DxN numpy array of corresponding D dimensional descriptors.\n",
    "    \"\"\"\n",
    "    if pts is None or desc is None:\n",
    "      print('PointTracker: Warning, no points were added to tracker.')\n",
    "      return\n",
    "    assert pts.shape[1] == desc.shape[1]\n",
    "    # Initialize last_desc.\n",
    "    if self.last_desc is None:\n",
    "      self.last_desc = np.zeros((desc.shape[0], 0))\n",
    "    # Remove oldest points, store its size to update ids later.\n",
    "    remove_size = self.all_pts[0].shape[1]\n",
    "    self.all_pts.pop(0)\n",
    "    self.all_pts.append(pts)\n",
    "    # Remove oldest point in track.\n",
    "    self.tracks = np.delete(self.tracks, 2, axis=1)\n",
    "    # Update track offsets.\n",
    "    for i in range(2, self.tracks.shape[1]):\n",
    "      self.tracks[:, i] -= remove_size\n",
    "    self.tracks[:, 2:][self.tracks[:, 2:] < -1] = -1\n",
    "    offsets = self.get_offsets()\n",
    "    # Add a new -1 column.\n",
    "    self.tracks = np.hstack((self.tracks, -1*np.ones((self.tracks.shape[0], 1))))\n",
    "    # Try to append to existing tracks.\n",
    "    matched = np.zeros((pts.shape[1])).astype(bool)\n",
    "    matches = self.nn_match_two_way(self.last_desc, desc, self.nn_thresh)\n",
    "    for match in matches.T:\n",
    "      # Add a new point to it's matched track.\n",
    "      id1 = int(match[0]) + offsets[-2]\n",
    "      id2 = int(match[1]) + offsets[-1]\n",
    "      found = np.argwhere(self.tracks[:, -2] == id1)\n",
    "      if found.shape[0] > 0:\n",
    "        matched[int(match[1])] = True\n",
    "        row = int(found)\n",
    "        self.tracks[row, -1] = id2\n",
    "        if self.tracks[row, 1] == self.max_score:\n",
    "          # Initialize track score.\n",
    "          self.tracks[row, 1] = match[2]\n",
    "        else:\n",
    "          # Update track score with running average.\n",
    "          # NOTE(dd): this running average can contain scores from old matches\n",
    "          #           not contained in last max_length track points.\n",
    "          track_len = (self.tracks[row, 2:] != -1).sum() - 1.\n",
    "          frac = 1. / float(track_len)\n",
    "          self.tracks[row, 1] = (1.-frac)*self.tracks[row, 1] + frac*match[2]\n",
    "    # Add unmatched tracks.\n",
    "    new_ids = np.arange(pts.shape[1]) + offsets[-1]\n",
    "    new_ids = new_ids[~matched]\n",
    "    new_tracks = -1*np.ones((new_ids.shape[0], self.maxl + 2))\n",
    "    new_tracks[:, -1] = new_ids\n",
    "    new_num = new_ids.shape[0]\n",
    "    new_trackids = self.track_count + np.arange(new_num)\n",
    "    new_tracks[:, 0] = new_trackids\n",
    "    new_tracks[:, 1] = self.max_score*np.ones(new_ids.shape[0])\n",
    "    self.tracks = np.vstack((self.tracks, new_tracks))\n",
    "    self.track_count += new_num # Update the track count.\n",
    "    # Remove empty tracks.\n",
    "    keep_rows = np.any(self.tracks[:, 2:] >= 0, axis=1)\n",
    "    self.tracks = self.tracks[keep_rows, :]\n",
    "    # Store the last descriptors.\n",
    "    self.last_desc = desc.copy()\n",
    "    return\n",
    "\n",
    "  def get_tracks(self, min_length):\n",
    "    \"\"\" Retrieve point tracks of a given minimum length.\n",
    "    Input\n",
    "      min_length - integer >= 1 with minimum track length\n",
    "    Output\n",
    "      returned_tracks - M x (2+L) sized matrix storing track indices, where\n",
    "        M is the number of tracks and L is the maximum track length.\n",
    "    \"\"\"\n",
    "    if min_length < 1:\n",
    "      raise ValueError('\\'min_length\\' too small.')\n",
    "    valid = np.ones((self.tracks.shape[0])).astype(bool)\n",
    "    good_len = np.sum(self.tracks[:, 2:] != -1, axis=1) >= min_length\n",
    "    # Remove tracks which do not have an observation in most recent frame.\n",
    "    not_headless = (self.tracks[:, -1] != -1)\n",
    "    keepers = np.logical_and.reduce((valid, good_len, not_headless))\n",
    "    returned_tracks = self.tracks[keepers, :].copy()\n",
    "    return returned_tracks\n",
    "\n",
    "  def draw_tracks(self, out, tracks):\n",
    "    \"\"\" Visualize tracks all overlayed on a single image.\n",
    "    Inputs\n",
    "      out - numpy uint8 image sized HxWx3 upon which tracks are overlayed.\n",
    "      tracks - M x (2+L) sized matrix storing track info.\n",
    "    \"\"\"\n",
    "    # Store the number of points per camera.\n",
    "    pts_mem = self.all_pts\n",
    "    N = len(pts_mem) # Number of cameras/images.\n",
    "    # Get offset ids needed to reference into pts_mem.\n",
    "    offsets = self.get_offsets()\n",
    "    # Width of track and point circles to be drawn.\n",
    "    stroke = 1\n",
    "    # Iterate through each track and draw it.\n",
    "    for track in tracks:\n",
    "      clr = myjet[int(np.clip(np.floor(track[1]*10), 0, 9)), :]*255\n",
    "      for i in range(N-1):\n",
    "        if track[i+2] == -1 or track[i+3] == -1:\n",
    "          continue\n",
    "        offset1 = offsets[i]\n",
    "        offset2 = offsets[i+1]\n",
    "        idx1 = int(track[i+2]-offset1)\n",
    "        idx2 = int(track[i+3]-offset2)\n",
    "        pt1 = pts_mem[i][:2, idx1]\n",
    "        pt2 = pts_mem[i+1][:2, idx2]\n",
    "        p1 = (int(round(pt1[0])), int(round(pt1[1])))\n",
    "        p2 = (int(round(pt2[0])), int(round(pt2[1])))\n",
    "        cv2.line(out, p1, p2, clr, thickness=stroke, lineType=16)\n",
    "        # Draw end points of each track.\n",
    "        if i == N-2:\n",
    "          clr2 = (255, 0, 0)\n",
    "          cv2.circle(out, p2, stroke, clr2, -1, lineType=16)\n",
    "\n",
    "class VideoStreamer(object):\n",
    "  \"\"\" Class to help process image streams. Three types of possible inputs:\"\n",
    "    1.) USB Webcam.\n",
    "    2.) A directory of images (files in directory matching 'img_glob').\n",
    "    3.) A video file, such as an .mp4 or .avi file.\n",
    "  \"\"\"\n",
    "  def __init__(self, basedir, camid, height, width, skip, img_glob):\n",
    "    self.cap = []\n",
    "    self.camera = False\n",
    "    self.video_file = False\n",
    "    self.listing = []\n",
    "    self.sizer = [height, width]\n",
    "    self.i = 0\n",
    "    self.skip = skip\n",
    "    self.maxlen = 1000000\n",
    "    # If the \"basedir\" string is the word camera, then use a webcam.\n",
    "    if basedir == \"camera/\" or basedir == \"camera\":\n",
    "      print('==> Processing Webcam Input.')\n",
    "      self.cap = cv2.VideoCapture(camid)\n",
    "      self.listing = range(0, self.maxlen)\n",
    "      self.camera = True\n",
    "    else:\n",
    "      # Try to open as a video.\n",
    "      self.cap = cv2.VideoCapture(basedir)\n",
    "      lastbit = basedir[-4:len(basedir)]\n",
    "      if (type(self.cap) == list or not self.cap.isOpened()) and (lastbit == '.mp4'):\n",
    "        raise IOError('Cannot open movie file')\n",
    "      elif type(self.cap) != list and self.cap.isOpened() and (lastbit != '.txt'):\n",
    "        print('==> Processing Video Input.')\n",
    "        num_frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        self.listing = range(0, num_frames)\n",
    "        self.listing = self.listing[::self.skip]\n",
    "        self.camera = True\n",
    "        self.video_file = True\n",
    "        self.maxlen = len(self.listing)\n",
    "      else:\n",
    "        print('==> Processing Image Directory Input.')\n",
    "        search = os.path.join(basedir, img_glob)\n",
    "        self.listing = glob.glob(search)\n",
    "        self.listing.sort()\n",
    "        self.listing = self.listing[::self.skip]\n",
    "        self.maxlen = len(self.listing)\n",
    "        if self.maxlen == 0:\n",
    "          raise IOError('No images were found (maybe bad \\'--img_glob\\' parameter?)')\n",
    "\n",
    "  def read_image(self, impath, img_size):\n",
    "    \"\"\" Read image as grayscale and resize to img_size.\n",
    "    Inputs\n",
    "      impath: Path to input image.\n",
    "      img_size: (W, H) tuple specifying resize size.\n",
    "    Returns\n",
    "      grayim: float32 numpy array sized H x W with values in range [0, 1].\n",
    "    \"\"\"\n",
    "    grayim = cv2.imread(impath, 0)\n",
    "    if grayim is None:\n",
    "      raise Exception('Error reading image %s' % impath)\n",
    "    # Image is resized via opencv.\n",
    "    interp = cv2.INTER_AREA\n",
    "    grayim = cv2.resize(grayim, (img_size[1], img_size[0]), interpolation=interp)\n",
    "    grayim = (grayim.astype('float32') / 255.)\n",
    "    return grayim\n",
    "\n",
    "  def next_frame(self):\n",
    "    \"\"\" Return the next frame, and increment internal counter.\n",
    "    Returns\n",
    "       image: Next H x W image.\n",
    "       status: True or False depending whether image was loaded.\n",
    "    \"\"\"\n",
    "    if self.i == self.maxlen:\n",
    "      return (None, False)\n",
    "    if self.camera:\n",
    "      ret, input_image = self.cap.read()\n",
    "      if ret is False:\n",
    "        print('VideoStreamer: Cannot get image from camera (maybe bad --camid?)')\n",
    "        return (None, False)\n",
    "      if self.video_file:\n",
    "        self.cap.set(cv2.CAP_PROP_POS_FRAMES, self.listing[self.i])\n",
    "      input_image = cv2.resize(input_image, (self.sizer[1], self.sizer[0]),\n",
    "                               interpolation=cv2.INTER_AREA)\n",
    "      input_image = cv2.cvtColor(input_image, cv2.COLOR_RGB2GRAY)\n",
    "      input_image = input_image.astype('float')/255.0\n",
    "    else:\n",
    "      image_file = self.listing[self.i]\n",
    "      input_image = self.read_image(image_file, self.sizer)\n",
    "    # Increment internal counter.\n",
    "    self.i = self.i + 1\n",
    "    input_image = input_image.astype('float32')\n",
    "    return (input_image, True)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  \n",
    "  ###################\n",
    "  #choose you values here\n",
    "  ####################\n",
    "#     D:/Swayatt Robots/work/work done/dataSets/test/annoted_video.mp4\n",
    "#  D:/Swayatt Robots/work/work done/dataSets/new data/saransh/video.mp4\n",
    "  impath = 'D:/Swayatt Robots/work/work done/dataSets/new data/saransh/sample_annotated1/video.mp4'\n",
    "  camid=0\n",
    "  H=240\n",
    "  W = 240\n",
    "  skip = True\n",
    "# D:/Swayatt Robots/work/work done/dataSets/new data/saransh/sample_annotated1/\n",
    "#annoted_video.mp4\n",
    "  img_glob ='D:/Swayatt Robots/work/work done/dataSets/new data/saransh/sample_annotated1/video.mp4'\n",
    "  img_size = (280,240)\n",
    "  weights_path = 'superpoint_v1.pth'\n",
    "  nms_dist = 4\n",
    "  conf_thresh = 0.001\n",
    "  nn_thresh = 0.7\n",
    "  max_length = 10\n",
    "  min_length = 2\n",
    "  #tmp = VideoStreamer('D:/',0,56,56,True,'')\n",
    "\n",
    "\n",
    "  # This class helps load input images from different sources.\n",
    "  vs = VideoStreamer(impath, camid, H, W, skip, img_glob)\n",
    "\n",
    "  print('==> Loading pre-trained network.')\n",
    "  # This class runs the SuperPoint network and processes its outputs.\n",
    "  fe = SuperPointFrontend(weights_path=weights_path,\n",
    "                          nms_dist=nms_dist,\n",
    "                          conf_thresh=conf_thresh,\n",
    "                          nn_thresh=nn_thresh,\n",
    "                          cuda=True)\n",
    "  print('==> Successfully loaded pre-trained network.')\n",
    "\n",
    "  # This class helps merge consecutive point matches into tracks.\n",
    "  tracker = PointTracker(max_length, nn_thresh=fe.nn_thresh)\n",
    "\n",
    "    \n",
    "  # Create a window to display the demo.\n",
    "\n",
    "  win = 'SuperPoint Tracker'\n",
    "  cv2.namedWindow(win)\n",
    "\n",
    "\n",
    "  # Font parameters for visualizaton.\n",
    "  font = cv2.FONT_HERSHEY_DUPLEX\n",
    "  font_clr = (255, 255, 255)\n",
    "  font_pt = (4, 12)\n",
    "  font_sc = 0.4\n",
    "  \n",
    "  print('==> Running Demo.')\n",
    "  while True:\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Get a new image.\n",
    "    img, status = vs.next_frame()\n",
    "    if status is False:\n",
    "      break\n",
    "\n",
    "    # Get points and descriptors.\n",
    "    start1 = time.time()\n",
    "    pts, desc, heatmap = fe.run(img)\n",
    "    end1 = time.time()\n",
    "\n",
    "    # Add points and descriptors to the tracker.\n",
    "    tracker.update(pts, desc)\n",
    "    \n",
    "    # Get tracks for points which were match successfully across all frames.\n",
    "    tracks = tracker.get_tracks(min_length)\n",
    "\n",
    "    # Primary output - Show point tracks overlayed on top of input image.\n",
    "    out1 = (np.dstack((img, img, img)) * 255.).astype('uint8')\n",
    "    tracks[:, 1] /= float(fe.nn_thresh) # Normalize track scores to [0,1].\n",
    "    tracker.draw_tracks(out1, tracks)\n",
    "    \n",
    "    cv2.putText(out1, 'Point Tracks', font_pt, font, font_sc, font_clr, lineType=16)\n",
    "\n",
    "    # Extra output -- Show current point detections.\n",
    "#     out2 = (np.dstack((img, img, img)) * 255.).astype('uint8')\n",
    "    out2 = (np.dstack((img, img, img)) * 255.).astype('uint8')\n",
    "    for pt in pts.T:\n",
    "      pt1 = (int(round(pt[0])), int(round(pt[1])))\n",
    "      cv2.circle(out2, pt1, 1, (0, 255, 0), -1, lineType=16)\n",
    "    cv2.putText(out2, 'Raw Point Detections', font_pt, font, font_sc, font_clr, lineType=16)\n",
    "\n",
    "    # Extra output -- Show the point confidence heatmap.\n",
    "    if heatmap is not None:\n",
    "      min_conf = 0.00001\n",
    "      heatmap[heatmap < min_conf] = min_conf\n",
    "      heatmap = -np.log(heatmap)\n",
    "      heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 0.00001)\n",
    "      out3 = myjet[np.round(np.clip(heatmap*10, 0, 9)).astype('int'), :]\n",
    "      out3 = (out3*255).astype('uint8')\n",
    "    else:\n",
    "      out3 = np.zeros_like(out2)\n",
    "    cv2.putText(out3, 'Raw Point Confidences', font_pt, font, font_sc, font_clr, lineType=16)\n",
    "\n",
    "    # Resize final output.\n",
    "#     out = np.hstack((out1, out2, out3))\n",
    "    out = np.hstack((out1, out2))\n",
    "    out = cv2.resize(out, (1*5*W, 3*H))\n",
    "    #else:\n",
    "    #  out = cv2.resize(out1, (5*W, 5*H))\n",
    "    \n",
    "    \n",
    "    waitkey = 0\n",
    "    \n",
    "    # Display visualization image to screen.\n",
    "    cv2.imshow(win, out)\n",
    "    key = cv2.waitKey(waitkey) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        print('Quitting, \\'q\\' pressed.')\n",
    "        break\n",
    "\n",
    "    \"\"\"\n",
    "    # Optionally write images to disk.\n",
    "    if opt.write:\n",
    "      out_file = os.path.join(opt.write_dir, 'frame_%05d.png' % vs.i)\n",
    "      print('Writing image to %s' % out_file)\n",
    "      cv2.imwrite(out_file, out)\n",
    "    \"\"\"\n",
    "    end = time.time()\n",
    "    net_t = (1./ float(end1 - start))\n",
    "    total_t = (1./ float(end - start))\n",
    "    if True:\n",
    "      print('Processed image %d (net+post_process: %.2f FPS, total: %.2f FPS).'\\\n",
    "            % (vs.i, net_t, total_t))\n",
    "\n",
    "  # Close any remaining windows.\n",
    "  cv2.destroyAllWindows()\n",
    "\n",
    "  print('==> Finshed Demo.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
